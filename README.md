# Floyd-Warshall_HUA-PCA-2017-2018
Parallel Implementation of Floyd-Warshall Algorithm using OpenMP and MPI.

## Authors
[Evan Markou](https://github.com/EvanMark), [George Alexandridis](https://github.com/galexandridis)

# Explanation

## Table of Contents
1. [Sequential Implementation]()
2. [OpenMP &amp; MPI]()
    1. [OpenMP Implementation]()
    2. [MPI Implementation]()
3. [Time &amp; Acceleration Comparison]()
4. [Conclusion]()

## Sequential Implementation
Για την υπολογισμό των ελάχιστων μονοπατιών ανάμεσα σε όλα τα ζεύγη ενός γράφου με dist Ν κόμβους, χρησιμοποιήθηκε ο αλγόριθμος Floyd-Warshall. Ο αλγόριθμος συγκρίνει όλα τα πιθανά μονοπάτια σε όλο το γράφο μέσω ενός ζεύγους κορυφών. Έχει πολυπλοκότητα Ν³ λόγω του τριπλού loop που υλοποιεί. Μέσα στα nested loops χρειάστηκε επιπλέον if conditions έτσι ώστε να σιγουρευτεί ο αλγόριθμος, ότι η ένωση (i,k,j) υπάρχει. Ελέγχει, δηλαδή, ότι υπάρχει k το οποίο ενώνεται άμεσα με i, j αντίστοιχα. Κύρια λειτουργία του αλγορίθμου υλοποιήθηκε στο παρακάτω κομμάτι κώδικα. 
```
time(&start);
//Calculate minimum distance paths
//Using the Floyd Warshall algorithm
for(k=0; k<n; k++)
	for(i=0; i<n; i++)
		for(j=0; j<n; j++)
			if ((dist[i][k] * dist[k][j] != 0) && (i != j))
				if(dist[i][j] > dist[i][k] + dist[k][j] || dist[i][j] == 0)
					dist[i][j] = dist[i][k] + dist[k][j];
			
time(&end);
```
Τρέχοντας τον αλγόριθμο για κόμβους n=10, 100, 1000 ,5000 αντίστοιχα, είχαμε τα παρακάτω αποτελέσματα.

| **Number of Nodes** | **Seconds** |
| --- | --- |
| 10 | 0.00 |
| 100 | 0.00 |
| 1000 | 7.00 |
| 5000 | 768.00 |

## OpenMP &amp; MPI
Το OpenMP είναι μια πλατφόρμα προγραμματισμού που επιτρέπει το παραλληλισμό του κώδικα σε ένα ομοιογενές κοινόχρηστο σύστημα μνήμης (π.χ. έναν επεξεργαστή πολλαπλών πυρήνων). Για παράδειγμα, θα μπορούσαμε να παραλληλοποιήσουμε ένα σύνολο λειτουργιών σε έναν επεξεργαστή πολλών πυρήνων όπου οι πυρήνες μοιράζονται μνήμη μεταξύ τους. Αυτή η μνήμη περιλαμβάνει μνήμη cache, μνήμη RAM, μνήμη σκληρού δίσκου κλπ. Αναφέρεται, επίσης, ότι η επικοινωνία είναι εύκολη και σχετικά φθηνή.

Το MPI (βασισμένο στη διεπαφή μετάδοσης μηνυμάτων) είναι επίσης πλατφόρμα προγραμματισμού, αλλά παρέχει την δυνατότητα παραλληλισμού κώδικα μέσω ενός (μη) ομοιογενούς κατανεμημένου συστήματος (π.χ. ενός υπερυπολογιστή). Για παράδειγμα, είναι δυνατό να παραλληλισθεί ένα ολόκληρο πρόγραμμα μέσω ενός δικτύου υπολογιστών ή κόμβων που επικοινωνούν μέσω του ίδιου δικτύου. Δεδομένου ότι αυτοί οι κόμβοι είναι ουσιαστικά υπολογιστές, έχουν τη δική τους διάταξη μνήμης και το δικό τους σύνολο πυρήνων (βλ. Περιγραφή OpenMP). Η επικοινωνία μεταξύ των κόμβων, σε σύγκριση με τα συστήματα κοινής μνήμης, μπορεί να είναι δύσκολη και συνήθως δαπανηρή.

Παρακάτω αναφέρονται οι τρόποι που υλοποιήθηκαν σε OpenMP και MPI αντίστοιχα, για τη εκτέλεση του αλγορίθμου Floyd Warshall παράλληλα.

### OpenMP Implementation
Το OpenMP μας δίνει τη δυνατότητα να παραλληλοποιήσουμε κώδικα με χρήση ακολουθιακών γλωσσών προγραμματισμού. Καλώντας τη βιβλιοθήκη omp.h παρέχονται ρουτίνες που παραλληλοποιούν διαδικασίες που χρίζονται με υψηλή πολυπλοκότητα. Οπότε θα πρέπει να χρησιμοποιούνται όταν ο ακολουθιακός κώδικας είναι resource intensive, γιατί σε άλλη περίπτωση το overhead θα αποβεί μοιραίο.  Στην εκτέλεση δημιουργούνται, εφόσον δεν τα έχουμε ορίσει κάπου, τόσα threads όσες είναι και οι CPUs του μηχανήματος, το οποίο είναι και ιδανικό(optimal). Κύρια αλγοριθμική λειτουργία παρατίθεται ακολούθως.
```
time(&start);
//Calculate minimum distance paths
//Using omp parallel for, it partitions the loop into the threads (as many as the CPUs) and runs the algorithm
#pragma omp parallel for private(i,j,k) shared(dist)
for(k=0; k<n; k++) 
	for(i=0; i<n; i++)
		for(j=0; j<n; j++)
			if ((dist[i][k] * dist[k][j] != 0) && (i != j))
				if(dist[i][j] > dist[i][k] + dist[k][j] || dist[i][j] == 0)
					dist[i][j] = dist[i][k] + dist[k][j];
		
time(&end);
```

Τα αποτελέσματα που παράχθηκαν από τη λειτουργία του OpenMP φαίνονται στο παρακάτω πίνακα.

| **Number of Nodes** | **Seconds** |
| --- | --- |
| 10 | 0.00 |
| 100 | 0.00 |
| 1000 | 3.00 |
| 5000 | 453.0 |

### MPI Implementation

Το MPI είναι ένας εναλλακτικός τρόπος παραλληλίας ενός ακολουθιακού κώδικα. Καλώντας την βιβλιοθήκη mpi.h, παρέχονται μηχανισμοί για την ανταλλαγή μηνυμάτων μέσω δικτύου ανάμεσα στα μηχανήματα του συστήματος. Ο ακολουθιακός κώδικας χωρίζεται σε 2 τμήματα.

Το ένα τμήμα(rank=MASTER) αναφέρεται στην διεργασία master, η οποία αρχικοποιεί τον πίνακα με τις αποστάσεις και τον στέλνει (με την MPI\_Send) στις υπόλοιπες διεργασίες(slaves). Λαμβάνει τα αποτελέσματα από την κάθε διεργασία, μέχρι την στιγμή την οποία όλες οι διεργασίες έχουν τελειώσει και  είναι ανενεργές. Μέσα σε αυτόν τον βρόχο για κάθε αποτέλεσμα που παίρνει επιλέγει την μικρότερη απόσταση μεταξύ του αποτελέσματος και της προηγούμενης τιμής της συγκεκριμένης απόστασης και ανανεώνει τον πίνακα. Η υλοποίηση του δίνεται παρακάτω.

 ```
 if (my_rank == MASTER) {
  double t1,t2;
  int disable=0,t=3;
  int result[t];
	//Initiate the dist with random values from 0-99
	for(i=0; i<n; i++)
		for(j=0; j<n; j++)
			if(i==j)
				dist[i][j] = 0;
			else
				dist[i][j] = rand()%100;
	//Print initial distances
	showDistances(dist);
	t1 = MPI_Wtime();
	for(i=1;i<num_procs;i++)
		MPI_Send(&(dist[0][0]),n*n,MPI_INT,i,WORKTAG,MPI_COMM_WORLD); //send the array dist in every machine
		//send takes the first argument [0][0] and the size of the array n*n and fills the array appropriately
	do {
		 MPI_Recv(&result,t,MPI_INT,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,&status);
		 if (status.MPI_TAG == DIETAG) //if received DIETAG(0) means that every other slaves has done and finish the whole process
		    disable++;
		 else
		    if (dist[result[1]][result[2]]>result[0]) //do a final check in results. Works like a reduce function
		        dist[result[1]][result[2]]=result[0];
	} while (disable < num_procs-1);	
	t2 = MPI_Wtime();
	//print the final distances
	showDistances(dist);
	printf("Total Elapsed Time %f sec\n", difftime(t2, t1));
}
 ```

Το δεύτερο τμήμα αναφέρεται σε όλες τις διεργασίες slaves, οι οποίες λαμβάνουν τον πίνακα που τους στέλνει η master. Το τμήμα που θα γίνει ο διαχωρισμός για την παραλληλοποίηση είναι το εξωτερικό loop. Η κάθε διεργασία slave παίρνει ένα τμήμα του loop (με δείκτη k). Υπολογίζει το τμήμα με την χρήση του rank της. Υπολογίζει όλες τις τίμες που της ανατέθηκαν (βρίσκει τις μικρότερες αποστάσεις) και στέλνει την κάθε τιμή στην διεργασία master. Μόλις τελειώσει πρέπει να ενημερώσει την διεργασία master για να μπορέσει να τερματιστεί την κατάλληλη στιγμή όλο το πρόγραμμα. Η υλοποίηση του δίνεται παρακάτω.

 ```
 /* workers code*/
else{
    int i, j, k,t=3;
    int out[t];
    MPI_Recv(&(dist[0][0]),n*n,MPI_INT,MASTER,MPI_ANY_TAG,MPI_COMM_WORLD,&status);
    if(my_rank+1!=num_procs)
        remain=0;
    for (k = slice*(my_rank-1); k < slice*(my_rank-1)+slice+remain; ++k) // slice the k alongside the ranks(slaves) 
        for (i = 0; i < n; ++i) //use of remain for the last slice
            for (j = 0; j < n; ++j)
                /* If i and j are different nodes and if
                    the paths between i and k and between
                    k and j exist, do */
                if ((dist[i][k] * dist[k][j] != 0) && (i != j))
                    /* See if you can't get a shorter path
                        between i and j by interspacing
                        k somewhere along the current
                        path */
                    if ((dist[i][k] + dist[k][j] < dist[i][j]) || (dist[i][j] == 0)){
                        dist[i][j] = dist[i][k] + dist[k][j];
                        out[0]=dist[i][j];
                        out[1]=i;
                        out[2]=j;
                        MPI_Send(&out,t,MPI_INT,MASTER,0,MPI_COMM_WORLD); //send back to master the calculated distance
                    }
    MPI_Send(0,0,MPI_INT,MASTER,DIETAG,MPI_COMM_WORLD);
}
 ```

Τα αποτελέσματα των χρόνων εκτέλεσης του αλγορίθμου φαίνονται παρακάτω.

| **Number of Nodes** | **Seconds** |
| --- | --- |
| 10 | 0.00 |
| 100 | 0.00 |
| 1000 | 7.00 |
| 5000 | 792.00 |

## Time &amp; Acceleration Comparison

Για να παραχθούν σωστά αποτελέσματα χρησιμοποιήθηκε το ίδιο seed και στις 3 υλοποιήσεις έτσι ώστε να παραχθεί ο ίδιος πίνακας αποστάσεων κάθε φορά με σκοπό να ελεγχθεί η ορθότητα της κάθε υλοποίησης και να μπορεί να γίνει δίκαιη σύγκριση μεταξύ περιπτώσεων.

Στην υλοποίηση του Ακολουθιακού αλγορίθμου και στην υλοποίηση του παράλληλου με την χρήση του OpenMP για την μέτρηση του χρόνου εκτέλεσης χρησιμοποιείται η time. Στην υλοποίηση όμως με την χρήση του MPI για την ορθή καταμέτρηση του χρόνου εκτέλεσης χρησιμοποιείται η MPI\_Wtime (για να αποφεύγονται περιπτώσεις μη συγχρονισμένων ρολογιών κτλ).

Στην περίπτωση του MPI έγιναν προσπάθειες για εκτέλεση σε πολλαπλά μηχανήματα στο εργαστήριο του 2ου. Τα μηχανήματα όμως δεν ανταποκρίνονταν. Για αυτόν τον λόγο έγινε εκτέλεση σε ένα μόνο τοπικό μηχάνημα για να διαπιστωθεί η ορθή λειτουργία του αλγορίθμου.

Το μηχάνημα που χρησιμοποιείται και στις 3 περιπτώσεις έχει 4 νήματα.

Τα αποτελέσματα των χρόνων εκτέλεσης για κάθε περίπτωση φαίνονται συγκεντρωτικά στον παρακάτω πίνακα.

|   | **n=10** | **n=100** | **n=1000** | **n=5000** |
| --- | --- | --- | --- | --- |
| **Ακολουθιακός** | 0.0 sec | 0.0 sec | 7.0 sec | 768.0 sec |
| **OpenMP** | 0.0 sec | 0.0 sec | 3.0 sec | 453.0 sec |
| **MPI** | 0.0 sec | 0.0 sec | 7.0 sec | 792.0 sec |

Για τον υπολογισμό της επιτάχυνσης για κάθε περίπτωση χρησιμοποιείται ο τύπος:

`S(p) = tₛ/tₚ`, όπου `tₛ` ο χρόνος εκτέλεσης του ακολουθιακού αλγορίθμου και `tₚ` ο χρόνος εκτέλεσης του παράλληλου. Όλες οι επιταχύνσεις φαίνονται στον παρακάτω πίνακα.

|   | n=10 | n=100 | n=1000 | n=5000 |
| --- | --- | --- | --- | --- |
| OpenMP | 0 | 0 | 2.33 | 1.70 |
| MPI | 0 | 0 | 1 | 0.97 |

## Conclusion

Από τις συγκρίσεις μεταξύ των χρόνων εκτέλεσης και των επιταχύνσεων παρατηρούμε πως υπάρχει μια σημαντική βελτίωση στην απόδοση με την χρήση του OpenMP σε σύγκριση με τον ακολουθιακό αλγόριθμο.

Στην περίπτωση του MPI, λόγω της αδυναμίας εκτέλεσης σε πολλαπλά μηχανήματα, τα αποτελέσματα δεν είναι αντιπροσωπευτικά. Αλλά παρατηρούμε πως υπάρχει επιβράδυνση σε σχέση με το ακολουθιακό πρόγραμμα. Αυτή η επιβάρυνση οφείλεται στον χρόνο που χρειάζεται για την ανταλλαγή μηνυμάτων (που περιέχουν τα δεδομένα) μεταξύ διεργασιών.
